---
title: "Replication of The Illusion of Consensus study (Experiment 5) by Yousif, Aboody & Keil (2019, Psychological Science)"
author: "Ayodele Dada (ayo.dada@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

[Link to the repository in psych251](https://github.com/psych251/yousif2019.git)


[Link to Qualtrics paradigm](https://stanforduniversity.qualtrics.com/jfe/form/SV_77YSqZE4iULPRK5)

[Link to preregistration](https://osf.io/vcmdp/)

## Introduction

As we navigate a world with a surfeit of information on almost anything, we are sometimes interested in the trustworthiness of some claims and reports. This matters to me both as a PhD student in the social area of psychology and because not all our knowledge of the world will come from personal experience. We depend to some extent on the information we get from primary and secondary sources. A respectable body of classic literature in social psychology has underlined the role of consensus in informing how confident we feel about a claim. From conformity studies to studies of social influence, consensus assists us in deciding what to believe or how to behave in several social situations. There is however a potential flaw to this broad use of consensus.
What if everyone we ask about a claim all seem to find it trustworthy despite all getting their information from a single primary source? If we perceived consensus on an issue simply because many secondary sources said the same thing, would our perception of consensus change if we were informed that there was only one primary source? This has real world implications today given recent events which have made their way into public discourse. For example, what exactly qualifies as "fake news" in these fraught political times? If many people repeat the same thing as told to all of them by one person, how confident would we be that we could trust such information? Are the scientists who make almost apocalyptic predictions about climate change simply alarmists? Did they reach their conclusions independently or are they victims of the cumulative nature of science to perpetuate erroneous conclusions from a single study? Conversely, are the naysayers of climate change who depend on the assertions of a single source worth taking seriously?
To propose some answers to these questions, this study replicated original work by Yousif, Aboody and Keil (2019) who suggested that the illusion of consensus was significant enough to influence people's confidence in a claim or story. The illusion of consensus refers to a subject's tendency to be overconfident in a claim from several secondary sources based on a single primary source (false consensus). This leads them to underestimate the difference between such claims and one from several primary sources (true consensus). A primary source is the witness of the event which is the basis of their claim while a secondary source simply reports the claims of the primary source. 
Based on the foregoing, the current study will focus on two hypotheses:

*H1: Participants will be more confident of the news reports in the true consensus condition than in the false consensus condition*

*H2: Participants' confidence in the news reports will be greater in the false consensus condition than in the no consensus condition*

## Methods
The research adopted a 3-group between subjects design. It was an experiment with three conditions namely: 
True consensus (Experimental group)

False consensus (Experimental group)

No consensus (Control group)

The variables of interest were number of primary sources making the same claim (independent variable) and the confidence of the participant in the claim (dependent variable). There were 3 levels of the independent variable corresponding to the three experimental conditions: four primary sources with the same claim and one opposing claim, four secondary sources with the same claim based on a single primary source with one opposing claim, and one primary source with a claim alongside one opposing claim. The dependent variable was reported by participants on a scale of 0 to 100.

### Power Analysis
The following power analysis was computed using the pwr.t.test function in the 'pwr' package in R. Arguments in the function were informed by medium estimates of effect size (d = 0.5) due to the experimental nature of the study, recommended minimum power (power = 0.85), significance level of 0.05, independent samples t-tests and directional hypotheses.
```{r Power analysis}
library(pwr)
pwr.t.test(d = 0.5,n = NULL,power = 0.85, sig.level = 0.05, type = "two.sample",alternative = "greater")

```


### Planned Sample

Based on the power analysis in the previous section, at least 60 participants will be randomly assigned to each group or experimental condition. This translates to N >= 180 for the current study. All participants would be recruited from Amazon's Mechanical Turk after completing an online survey and were all resident in the United States. As was the case in the original study, any participant who failed the attention checks at the end of the experiment would be excluded from the study. In the event that any participants fail these attention checks, other participants will be recruited untill the desired sample size is met.
In the original study, 80 participants were randomly assigned to each condition for this experiment making a total of N = 240. 

### Materials

"Materials consisted of fake news articles about a bear sighting on a local high school campus. Four of the news articles claimed to have witnessed the bear, and one took a negative stance (the bear sightings were false). All report cited a single primary source (the name of a witness). Each news article was from a different news outlet from the others" (Yousif, Aboody & Keil, 2019).


### Procedure	
(The procedure is identical to the original study except for some minor changes as indicated by the italicized words.)

"*Sixty* participants were randomly assigned to three conditions: a true-consensus condition, a no-consensus condition, and a false-consensus condition. In the true-consensus condition, participants read all five news reports (four positive, one negative). Because each news report cited a unique primary source, participants heard from five primary sources.
Participants in the false-consensus condition also read all five news reports, except that the four news reports claiming to have seen a bear all cited the same primary source. Thus, participants heard from only two primary sources overall (one affirming the sighting and one taking an opposing view). In the baseline no consensus condition, participants read only two news articles: the one no-sighting report and one positive sighting report randomly drawn from the four available positive sightings. In this condition, then, participants heard from two primary sources (as in the false-consensus condition, one took a positive stance and one took a negative stance)" (Yousif, Aboody & Keil, 2019).
"We expected that participants’ confidence ratings would be significantly higher in the true-consensus condition than in the no-consensus condition. Therefore, the critical question was where confidence ratings in the false-consensus condition would fall. If people tracked only the number of primary sources, then confidence ratings in the false-consensus condition should be identical to those in the no-consensus condition, because in both, only a single primary source was cited on each side of the argument. On the other hand, if people’s estimations of consensus relied only on the number of secondary sources, then confidence ratings in the false-consensus condition should be identical to those in the true-consensus condition, because both included four positive report and one negative report" (Yousif, Aboody & Keil, 2019).
"Across all conditions, the exact same information was presented in each report; all that varied (other than superficial changes in the language) was the number of unique primary sources cited across the articles. The news reports were presented in a randomized order, one at a time, to each participant " (Yousif, Aboody & Keil, 2019). 
"After reading the articles, participants read the following prompt, presented in plain text at the top of the screen: “Based on the news reports you saw, to what extent do you agree that *a bear was seen close to the school*? Please rate your agreement on the scale below from 0 (strongly disagree) to 100 (strongly agree).” Participants responded by clicking on a number line to indicate their confidence that the bear sighting was true and were prompted to confirm their answer before submitting it. On a separate screen, participants were then asked two questions" (Yousif, Aboody & Keil, 2019).
"First, they saw a list of 10 sources and were asked to indicate which had been cited in the news reports they read (there were no limits on the number of sources participants could select). Next, they saw a *list of five schools* and were asked to identify which one the news reports had been about. Participants who answered incorrectly were excluded and replaced. No other information was collected." (Yousif, Aboody & Keil, 2019)


### Analysis Plan
From the hypotheses, the analyses in this study will be identical to the original study in which straightforward independent t-tests were used to test both hypotheses. Just as was done in the original study, one sample t-tests may also be conducted to confirm that participants were able to identify the sources at above chance levels. The key statistical test will be the comparison between ratings in the False consensus and No consensus conditions. This study will however not eliminate the top and bottom quintiles of the ratings like the original study because we cannot be sure that the distribution in any condition will be normal. Excluding such apparently extreme ratings may not reflect the realities of human judgement.


### Differences from Original Study
One obvious difference between the current study and the original one will be the sample size. This should not result in any significant differences in outcome since the minimum power criterion for the study should be satisfied.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r echo=TRUE}
### Data Preparation

### Load Relevant Libraries and Functions
library(tidyverse)
library(DT) 
library(ggthemes) 
#library(wordbankr)
library(purrr)
library(shiny)
library(here)

#### Import data#### Prepare data for analysis - create columns etc.
"Data from Qualtrics was imported in CSV numeric format. The survey was modified in real time as responses were coming in from the participants. Out of almost 80 respondents, only 29 were useful for the pilot at the time of analysis."

IlluCon <- read_csv(here("Replication project_ Illusion of consensus_December 12, 2019_10.31.csv"))

IlluCon_light <- IlluCon %>%
  filter(AttCheck2 == 4) %>%
  select(c("Positive_1", "Positive_2", "Positive_3", "Positive_4", "Negative_1", "False_Consensus_1", "False_Consensus_2", "False_Consensus_3", "Rating_1"))

#### Data exclusion / filtering
"As specified in the procedure, participants who fail the attention check will be excluded. As in the original study, results from approximately 10% of the participants will be excluded"

IlluCon_light <- IlluCon_light %>%
  filter(Negative_1 == 4)


#IlluCon_light[is.na(IlluCon_light)] <- 0  #Change all NA to 0 so that numeric operations can be carried out on values
#this line didn't work, but clear what's meant
IlluCon_light <- IlluCon_light |> 
#   mutate(across(everything(), ~ifelse(is.na(.x),0,.x) |> ))
# 
# which(!is.numeric(IlluCon_light))     #Change every other column entry to numeric values
# 
# IlluCon_light$Positive_1 <- as.numeric(IlluCon_light$Positive_1)
# IlluCon_light$Positive_2 <- as.numeric(IlluCon_light$Positive_2)
# IlluCon_light$Positive_3 <- as.numeric(IlluCon_light$Positive_3)
# IlluCon_light$Positive_4 <- as.numeric(IlluCon_light$Positive_4)
# IlluCon_light$Negative_1 <- as.numeric(IlluCon_light$Negative_1)
# IlluCon_light$False_Consensus_1 <- as.numeric(IlluCon_light$False_Consensus_1)
# IlluCon_light$False_Consensus_2 <- as.numeric(IlluCon_light$False_Consensus_2)
# IlluCon_light$False_Consensus_3 <- as.numeric(IlluCon_light$False_Consensus_3)
# IlluCon_light$Rating_1 <- as.numeric(IlluCon_light$Rating_1)
# 
# f <- sapply(IlluCon_light, is.numeric)
# 
# which(f)
# IlluCon_light[, f]


IlluCon_light <- IlluCon_light %>%       #Indicate the conditions in the experiment
  mutate(ExpCondition1 = ifelse((Positive_1 + Positive_2 + Positive_3 + Positive_4) == 4,"TrueConsensus",ifelse(False_Consensus_1 == 4, "FalseConsensus", "NoConsensus")))


  

```




### Confirmatory analysis

```{r}
# new code written by VB
# original excludes upper and lower quintile by group 

#145 datapoints in original
# expect around 87 post exclusion

quintiles <- IlluCon_light |> select(ExpCondition1, Rating_1)|> 
  group_by(ExpCondition1) |> 
  summarize(low=quantile(Rating_1,probs=c(.2)),
            high=quantile(Rating_1, probs=c(.8)))

filtered <- IlluCon_light |>select(ExpCondition1, Rating_1)|> 
  left_join(quintiles) |> 
  filter(Rating_1>=low) |> 
  filter(Rating_1<= high)
  # it's unclear if we should include or exclude the equals ones!!

no_v_false <- filtered |> filter(ExpCondition1 %in% c("NoConsensus","FalseConsensus"))

t.test(Rating_1 ~ ExpCondition1, data=no_v_false)

no_v_false |> group_by(ExpCondition1) |>  summarize(m=mean(Rating_1), se=sd(Rating_1)/sqrt(n()))
```

The analyses as specified in the analysis plan.  
```{r echo=TRUE}
"The study will analyse the data using t-tests. The True Consensus level will be compared with False Consensus while the False Consensus will be compared with the No Consensus condition"

IlluCon_light_summary <- IlluCon_light[,c(9,10)] # This keeps only the columns required for the analysis

IlluCon_light_summary1 <- subset(IlluCon_light_summary, ExpCondition1 != "NoConsensus") # This keeps only the observations required for hypothesis 1 which compares True Consensus with False Consensus and drops the No Consensus observations

IlluCon_light_summary2 <- subset(IlluCon_light_summary, ExpCondition1 != "TrueConsensus") # This keeps only the observations required for hypothesis 2 which compares No Consensus with False Consensus and drops the True Consensus observations
```



```{r echo=TRUE}
True_hist <- subset(IlluCon_light_summary,ExpCondition1 == "TrueConsensus") # Histogram for the True Consensus condition
ggplot(data = True_hist, mapping = aes(x = Rating_1))+
  geom_histogram(fill = "purple", binwidth = 1)+
  labs(x = "Rating")+
  theme_bw()
```


The ratings in the True consensus condition have a left skew as expected. People are likely to be more confident of their ratings here and this is observed by most ratings being above 50%.


```{r echo=TRUE}

False_hist <- subset(IlluCon_light_summary,ExpCondition1 == "FalseConsensus") # Histogram for the False Consensus condition
ggplot(data = False_hist, mapping = aes(x = Rating_1))+
  geom_histogram(fill = "purple", binwidth = 1)+
  labs(x = "Rating")+
  theme_bw()

```


There is also a left skew in the False consensus condition but it is less pronounced than the True consensus distribution.



```{r echo=TRUE}
No_hist <- subset(IlluCon_light_summary,ExpCondition1 == "NoConsensus") # Histogram for the No Consensus condition
ggplot(data = No_hist, mapping = aes(x = Rating_1))+
  geom_histogram(fill = "purple", binwidth = 1)+
  labs(x = "Rating")+
  theme_bw()

```


The No consensus condition lacks a precise shape but also has a slight left skew. Participants may be quite confident of their judgment when required to make ratings and commit to them.


```{r echo=TRUE}                                                      
t.test(Rating_1 ~ ExpCondition1, data = IlluCon_light_summary1) # This T-test tests hypothesis 1 (True Consensus vs False Consensus)


Illusion_summary1 <- IlluCon_light_summary1 %>% # the names of the new data frame and the data frame to be summarised
  group_by(ExpCondition1) %>%   # the grouping variable
  summarise(mean_Rating = mean(Rating_1),  # calculates the mean of each group
            sd_Rating = sd(Rating_1), # calculates the standard deviation of each group
            n_Rating = n(),  # calculates the sample size per group
            SE_Rating = sd(Rating_1)/sqrt(n())) # calculates the standard error of each group

IllusionPlot1 <- ggplot(Illusion_summary1, aes(ExpCondition1, mean_Rating)) + 
                   geom_col(fill = "purple") +  
                   geom_errorbar(aes(ymin = mean_Rating - sd_Rating, ymax = mean_Rating + sd_Rating), width=0.2, fill)

IllusionPlot1 + labs(y="Confidence Rating", x = "Condition") + theme_classic()

```


```{r echo=TRUE}

t.test(Rating_1 ~ ExpCondition1, data = IlluCon_light_summary2) # This T-test tests hypothesis 2 (False Consensus vs No Consensus)


Illusion_summary2 <- IlluCon_light_summary2 %>% # the names of the new data frame and the data frame to be summarised
  group_by(ExpCondition1) %>%   # the grouping variable
  summarise(mean_Rating = mean(Rating_1),  # calculates the mean of each group
            sd_Rating = sd(Rating_1), # calculates the standard deviation of each group
            n_Rating = n(),  # calculates the sample size per group
            SE_Rating = sd(Rating_1)/sqrt(n())) # calculates the standard error of each group

IllusionPlot2 <- ggplot(Illusion_summary2, aes(ExpCondition1, mean_Rating)) + 
                   geom_col(fill = "purple") +  
                   geom_errorbar(aes(ymin = mean_Rating - sd_Rating, ymax = mean_Rating + sd_Rating), width=0.2, fill)

IllusionPlot2 + labs(y="Confidence Rating", x = "Condition") + theme_classic()

```

```{r}
mean_1 <- 63.03226
mean_2 <- 72.18000
sd_1 <- 24.22324
sd_2 <- 23.71574
n_1 <- 31
n_2 <- 50
a_level <- 0.05

mean_diff <- mean_1 - mean_2
pooled_sd_n <- ((n_1-1)*(sd_1**2)) + ((n_1-1)*(sd_2**2))
pooled_sd_n
pooled_sd_d <- ((n_1 + n_2) - 2)
pooled_sd_d
pooled_sd <- sqrt(pooled_sd_n/pooled_sd_d)
cohens_d <- mean_diff / pooled_sd
cohens_d
```


```{r}
mean_1 <- 63.03226
mean_2 <- 62.87500
sd_1 <- 24.22324
sd_2 <- 25.33114
n_1 <- 31
n_2 <- 64
a_level <- 0.05

mean_diff <- mean_1 - mean_2
pooled_sd_n <- ((n_1-1)*(sd_1**2)) + ((n_1-1)*(sd_2**2))
pooled_sd_n
pooled_sd_d <- ((n_1 + n_2) - 2)
pooled_sd_d
pooled_sd <- sqrt(pooled_sd_n/pooled_sd_d)
cohens_d <- mean_diff / pooled_sd
cohens_d
```






### Summary of Replication Attempt

After conducting the hypothesis tests, the following results were obtained.
 True consensus vs False consensus: t(63) = 1.67, p =.1009, d = 0.44
 False consensus vs No consensus: t(62) = 0.02, p =.98, d = 0.01
The key test was the comparison of the False consensus condition with the No consensus one and no significant difference was found before removing the top and bottom quintiles. But this study did not intend to remove the top and bottom quintiles because none of the rating distributions in any condition were expected to resemble a normal distribution. Based on the eventual finding in the original study, we conclude that this test did not replicate.
In the True consensus vs false consensus test, no significant difference was observed either. This is also not consistent with the original study as this test did not replicate.
  

### Commentary

From these analyses at the time of submission,there are a few key differences both from the original study and from the planned study.

The study sample was obtained from the MTurk population and 17 participants failed the attention checks. Eventually, the desired sample size for the study was not achieved and this may have led to non-replication of the original study.
Even if this intended sample was obtained, the results from the tests of hypothesis indicate that any additional participants would have all had to make extreme ratings in an opposite direction for the differences between the groups of interest to end up being significant.
The extreme ratings could have possibly occurred because of the stimuli used in the study. For example, the negative article contained a claim made by a senior who was knowledgeable about bears and could have carried more weight than the other positive articles made by juniors or regular students who admitted that they had never seen a bear before. This adds the confound of the expertise of the claimant. Thus the differences in vignette contexts among conditions could have influenced the outcome of the study.